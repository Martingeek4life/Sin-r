import subprocess
import argparse
import string
import chardet
import numpy as np
from scipy.spatial import distance
import gensim.downloader as api
from gensim.models import KeyedVectors
from huggingface_hub import hf_hub_download
# model_pretrained_it = KeyedVectors.load_word2vec_format(hf_hub_download(repo_id="Word2vec/wikipedia2vec_itwiki_20180420_300d", filename="itwiki_20180420_300d.txt"))

# model_pretrained_en = KeyedVectors.load_word2vec_format(hf_hub_download(repo_id="Word2vec/wikipedia2vec_enwiki_20180420_300d", filename="enwiki_20180420_300d.txt"))
def detect_encoding(file_path):
    with open(file_path, 'rb') as file:
        rawdata = file.read()
        result = chardet.detect(rawdata)
        return result['encoding']
        print("Detected encoding:", result['encoding'])

def preprocess_text(corpus_path, output_file):
    encodage = detect_encoding(corpus_path)
    with open(corpus_path, 'r', encoding=encodage) as file:
        text = file.read().lower()
        text = text.translate(str.maketrans('', '', string.punctuation))
    with open(output_file, 'w', encoding='utf-8') as clean_file:
        clean_file.write(text)

def get_W2V_words_from_corpus(file_path_corpus):
    words = []
    with open(file_path_corpus, 'r', encoding='utf-8') as file:
        for line in file:
            # Diviser chaque ligne en mots en utilisant un espace comme séparateur
            line_words = line.strip().split()
            # Ajouter chaque mot à la liste des mots
            words.extend(line_words)
        print("words fr: ", words)
    return words


def generate_pretrained_w2v_it_en(words_en, words_it):
    with open("target_embeddings.vec", "w", encoding="utf-8") as f_out:   
        f_out.write(f"{len(words_en)} 300\n")
        f_out.write(f"</s>  0.15866 0.19586 -0.057656 0.04733 0.074815 0.12513 -0.4357 0.068475 0.15304 0.17517 0.0085197 -0.038124 -0.42547 0.02871 0.0028513 0.29215 0.1057 -0.10044 -0.20926 -0.06027 0.13492 0.17282 0.025721 0.044686 -0.27167 -0.021629 0.1665 0.08608 -0.28912 0.077425 0.16541 0.082522 -0.059421 -0.05745 -0.1263 -0.22378 0.053809 -0.18196 -0.15237 0.01966 -0.11164 -0.035306 0.080649 0.1549 0.30811 -0.46448 0.24098 -0.023042 0.10101 -0.054135 -0.19616 -0.015913 -0.12336 0.091652 -0.26264 -0.10263 -0.0035233 -0.015146 0.061538 0.0096151 0.1538 0.11143 0.15237 0.21533 0.12433 -0.016997 -0.070833 0.29267 -0.1151 -0.26612 -0.16511 0.19579 0.0031382 0.071859 -0.044777 -0.1025 0.3128 0.0448 0.28893 -0.13248 0.17442 0.064696 0.17354 0.091692 0.2004 0.10974 0.33205 0.085334 -0.14237 -0.18808 0.15533 0.10611 0.4627 -0.29126 -0.27753 -0.298 -0.16399 0.10904 -0.090041 -0.1876 
a 0.20184 0.20213 -0.0068384 0.022617 0.12328 0.088441 -0.41992 0.14175 0.19088 0.17783 -0.023079 0.045697 -0.44857 0.0085456 -0.18944 0.28967 0.010752 -0.060041 -0.21285 0.030452 0.075598 0.22313 0.031058 -0.0073945 -0.33162 -0.044641 0.082925 0.047076 -0.32801 0.080591 0.17454 0.11608 -0.072836 -0.0085212 -0.041314 -0.33207 0.036962 -0.10414 -0.17075 -0.066349 -0.12384 0.0067861 0.0091623 0.16404 0.26195 -0.41773 0.31712 -0.051899 0.071353 -0.046528 -0.22659 -0.098429 -0.1104 0.16071 -0.29572 -0.094333 -0.1087 0.02074 0.083292 0.0092326 0.22107 0.16124 0.12575 0.22289 0.17441 -0.022528 -0.042505 0.25714 -0.14974 -0.23864 -0.12664 0.17441 0.10994 0.042998 -0.016403 0.042373 0.23609 0.016707 0.21512 0.027021 0.22128 -0.0038756 0.26512 0.10322 0.25804 -0.024487 0.28698 0.04929 -0.13001 -0.040532 0.12707 -0.018822 0.45939 -0.24385 -0.35178 -0.27267 -0.25421 0.13457 -0.041762 -0.22118 0.15866 0.19586 -0.057656 0.04733 0.074815 0.12513 -0.4357 0.068475 0.15304 0.17517 0.0085197 -0.038124 -0.42547 0.02871 0.0028513 0.29215 0.1057 -0.10044 -0.20926 -0.06027 0.13492 0.17282 0.025721 0.044686 -0.27167 -0.021629 0.1665 0.08608 -0.28912 0.077425 0.16541 0.082522 -0.059421 -0.05745 -0.1263 -0.22378 0.053809 -0.18196 -0.15237 0.01966 -0.11164 -0.035306 0.080649 0.1549 0.30811 -0.46448 0.24098 -0.023042 0.10101 -0.054135 -0.19616 -0.015913 -0.12336 0.091652 -0.26264 -0.10263 -0.0035233 -0.015146 0.061538 0.0096151 0.1538 0.11143 0.15237 0.21533 0.12433 -0.016997 -0.070833 0.29267 -0.1151 -0.26612 -0.16511 0.19579 0.0031382 0.071859 -0.044777 -0.1025 0.3128 0.0448 0.28893 -0.13248 0.17442 0.064696 0.17354 0.091692 0.2004 0.10974 0.33205 0.085334 -0.14237 -0.18808 0.15533 0.10611 0.4627 -0.29126 -0.27753 -0.298 -0.16399 0.10904 -0.090041 -0.1876 
a 0.20184 0.20213 -0.0068384 0.022617 0.12328 0.088441 -0.41992 0.14175 0.19088 0.17783 -0.023079 0.045697 -0.44857 0.0085456 -0.18944 0.28967 0.010752 -0.060041 -0.21285 0.030452 0.075598 0.22313 0.031058 -0.0073945 -0.33162 -0.044641 0.082925 0.047076 -0.32801 0.080591 0.17454 0.11608 -0.072836 -0.0085212 -0.041314 -0.33207 0.036962 -0.10414 -0.17075 -0.066349 -0.12384 0.0067861 0.0091623 0.16404 0.26195 -0.41773 0.31712 -0.051899 0.071353 -0.046528 -0.22659 -0.098429 -0.1104 0.16071 -0.29572 -0.094333 -0.1087 0.02074 0.083292 0.0092326 0.22107 0.16124 0.12575 0.22289 0.17441 -0.022528 -0.042505 0.25714 -0.14974 -0.23864 -0.12664 0.17441 0.10994 0.042998 -0.016403 0.042373 0.23609 0.016707 0.21512 0.027021 0.22128 -0.0038756 0.26512 0.10322 0.25804 -0.024487 0.28698 0.04929 -0.13001 -0.040532 0.12707 -0.018822 0.45939 -0.24385 -0.35178 -0.27267 -0.25421 0.13457 -0.041762 -0.22118 0.15866 0.19586 -0.057656 0.04733 0.074815 0.12513 -0.4357 0.068475 0.15304 0.17517 0.0085197 -0.038124 -0.42547 0.02871 0.0028513 0.29215 0.1057 -0.10044 -0.20926 -0.06027 0.13492 0.17282 0.025721 0.044686 -0.27167 -0.021629 0.1665 0.08608 -0.28912 0.077425 0.16541 0.082522 -0.059421 -0.05745 -0.1263 -0.22378 0.053809 -0.18196 -0.15237 0.01966 -0.11164 -0.035306 0.080649 0.1549 0.30811 -0.46448 0.24098 -0.023042 0.10101 -0.054135 -0.19616 -0.015913 -0.12336 0.091652 -0.26264 -0.10263 -0.0035233 -0.015146 0.061538 0.0096151 0.1538 0.11143 0.15237 0.21533 0.12433 -0.016997 -0.070833 0.29267 -0.1151 -0.26612 -0.16511 0.19579 0.0031382 0.071859 -0.044777 -0.1025 0.3128 0.0448 0.28893 -0.13248 0.17442 0.064696 0.17354 0.091692 0.2004 0.10974 0.33205 0.085334 -0.14237 -0.18808 0.15533 0.10611 0.4627 -0.29126 -0.27753 -0.298 -0.16399 0.10904 -0.090041 -0.1876 
a 0.20184 0.20213 -0.0068384 0.022617 0.12328 0.088441 -0.41992 0.14175 0.19088 0.17783 -0.023079 0.045697 -0.44857 0.0085456 -0.18944 0.28967 0.010752 -0.060041 -0.21285 0.030452 0.075598 0.22313 0.031058 -0.0073945 -0.33162 -0.044641 0.082925 0.047076 -0.32801 0.080591 0.17454 0.11608 -0.072836 -0.0085212 -0.041314 -0.33207 0.036962 -0.10414 -0.17075 -0.066349 -0.12384 0.0067861 0.0091623 0.16404 0.26195 -0.41773 0.31712 -0.051899 0.071353 -0.046528 -0.22659 -0.098429 -0.1104 0.16071 -0.29572 -0.094333 -0.1087 0.02074 0.083292 0.0092326 0.22107 0.16124 0.12575 0.22289 0.17441 -0.022528 -0.042505 0.25714 -0.14974 -0.23864 -0.12664 0.17441 0.10994 0.042998 -0.016403 0.042373 0.23609 0.016707 0.21512 0.027021 0.22128 -0.0038756 0.26512 0.10322 0.25804 -0.024487 0.28698 0.04929 -0.13001 -0.040532 0.12707 -0.018822 0.45939 -0.24385 -0.35178 -0.27267 -0.25421 0.13457 -0.041762 -0.22118\n")
        # load pre-trained word embeddings model:
        for word_en in words_en:
            try:
                embedding = model_pretrained_en[word_en]
                f_out.write(f"{word_en} {' '.join(map(str, embedding))}\n")
            except KeyError:
                # Si le mot n'existe pas dans le modèle, passez simplement à l'itération suivante
                continue
    with open("source_embeddings.vec", "w", encoding="utf-8") as f_out:
        f_out.write(f"{len(words_it)} 300\n")
        f_out.write(f"</s>  0.15866 0.19586 -0.057656 0.04733 0.074815 0.12513 -0.4357 0.068475 0.15304 0.17517 0.0085197 -0.038124 -0.42547 0.02871 0.0028513 0.29215 0.1057 -0.10044 -0.20926 -0.06027 0.13492 0.17282 0.025721 0.044686 -0.27167 -0.021629 0.1665 0.08608 -0.28912 0.077425 0.16541 0.082522 -0.059421 -0.05745 -0.1263 -0.22378 0.053809 -0.18196 -0.15237 0.01966 -0.11164 -0.035306 0.080649 0.1549 0.30811 -0.46448 0.24098 -0.023042 0.10101 -0.054135 -0.19616 -0.015913 -0.12336 0.091652 -0.26264 -0.10263 -0.0035233 -0.015146 0.061538 0.0096151 0.1538 0.11143 0.15237 0.21533 0.12433 -0.016997 -0.070833 0.29267 -0.1151 -0.26612 -0.16511 0.19579 0.0031382 0.071859 -0.044777 -0.1025 0.3128 0.0448 0.28893 -0.13248 0.17442 0.064696 0.17354 0.091692 0.2004 0.10974 0.33205 0.085334 -0.14237 -0.18808 0.15533 0.10611 0.4627 -0.29126 -0.27753 -0.298 -0.16399 0.10904 -0.090041 -0.1876 
a 0.20184 0.20213 -0.0068384 0.022617 0.12328 0.088441 -0.41992 0.14175 0.19088 0.17783 -0.023079 0.045697 -0.44857 0.0085456 -0.18944 0.28967 0.010752 -0.060041 -0.21285 0.030452 0.075598 0.22313 0.031058 -0.0073945 -0.33162 -0.044641 0.082925 0.047076 -0.32801 0.080591 0.17454 0.11608 -0.072836 -0.0085212 -0.041314 -0.33207 0.036962 -0.10414 -0.17075 -0.066349 -0.12384 0.0067861 0.0091623 0.16404 0.26195 -0.41773 0.31712 -0.051899 0.071353 -0.046528 -0.22659 -0.098429 -0.1104 0.16071 -0.29572 -0.094333 -0.1087 0.02074 0.083292 0.0092326 0.22107 0.16124 0.12575 0.22289 0.17441 -0.022528 -0.042505 0.25714 -0.14974 -0.23864 -0.12664 0.17441 0.10994 0.042998 -0.016403 0.042373 0.23609 0.016707 0.21512 0.027021 0.22128 -0.0038756 0.26512 0.10322 0.25804 -0.024487 0.28698 0.04929 -0.13001 -0.040532 0.12707 -0.018822 0.45939 -0.24385 -0.35178 -0.27267 -0.25421 0.13457 -0.041762 -0.22118 0.15866 0.19586 -0.057656 0.04733 0.074815 0.12513 -0.4357 0.068475 0.15304 0.17517 0.0085197 -0.038124 -0.42547 0.02871 0.0028513 0.29215 0.1057 -0.10044 -0.20926 -0.06027 0.13492 0.17282 0.025721 0.044686 -0.27167 -0.021629 0.1665 0.08608 -0.28912 0.077425 0.16541 0.082522 -0.059421 -0.05745 -0.1263 -0.22378 0.053809 -0.18196 -0.15237 0.01966 -0.11164 -0.035306 0.080649 0.1549 0.30811 -0.46448 0.24098 -0.023042 0.10101 -0.054135 -0.19616 -0.015913 -0.12336 0.091652 -0.26264 -0.10263 -0.0035233 -0.015146 0.061538 0.0096151 0.1538 0.11143 0.15237 0.21533 0.12433 -0.016997 -0.070833 0.29267 -0.1151 -0.26612 -0.16511 0.19579 0.0031382 0.071859 -0.044777 -0.1025 0.3128 0.0448 0.28893 -0.13248 0.17442 0.064696 0.17354 0.091692 0.2004 0.10974 0.33205 0.085334 -0.14237 -0.18808 0.15533 0.10611 0.4627 -0.29126 -0.27753 -0.298 -0.16399 0.10904 -0.090041 -0.1876 
a 0.20184 0.20213 -0.0068384 0.022617 0.12328 0.088441 -0.41992 0.14175 0.19088 0.17783 -0.023079 0.045697 -0.44857 0.0085456 -0.18944 0.28967 0.010752 -0.060041 -0.21285 0.030452 0.075598 0.22313 0.031058 -0.0073945 -0.33162 -0.044641 0.082925 0.047076 -0.32801 0.080591 0.17454 0.11608 -0.072836 -0.0085212 -0.041314 -0.33207 0.036962 -0.10414 -0.17075 -0.066349 -0.12384 0.0067861 0.0091623 0.16404 0.26195 -0.41773 0.31712 -0.051899 0.071353 -0.046528 -0.22659 -0.098429 -0.1104 0.16071 -0.29572 -0.094333 -0.1087 0.02074 0.083292 0.0092326 0.22107 0.16124 0.12575 0.22289 0.17441 -0.022528 -0.042505 0.25714 -0.14974 -0.23864 -0.12664 0.17441 0.10994 0.042998 -0.016403 0.042373 0.23609 0.016707 0.21512 0.027021 0.22128 -0.0038756 0.26512 0.10322 0.25804 -0.024487 0.28698 0.04929 -0.13001 -0.040532 0.12707 -0.018822 0.45939 -0.24385 -0.35178 -0.27267 -0.25421 0.13457 -0.041762 -0.22118 0.15866 0.19586 -0.057656 0.04733 0.074815 0.12513 -0.4357 0.068475 0.15304 0.17517 0.0085197 -0.038124 -0.42547 0.02871 0.0028513 0.29215 0.1057 -0.10044 -0.20926 -0.06027 0.13492 0.17282 0.025721 0.044686 -0.27167 -0.021629 0.1665 0.08608 -0.28912 0.077425 0.16541 0.082522 -0.059421 -0.05745 -0.1263 -0.22378 0.053809 -0.18196 -0.15237 0.01966 -0.11164 -0.035306 0.080649 0.1549 0.30811 -0.46448 0.24098 -0.023042 0.10101 -0.054135 -0.19616 -0.015913 -0.12336 0.091652 -0.26264 -0.10263 -0.0035233 -0.015146 0.061538 0.0096151 0.1538 0.11143 0.15237 0.21533 0.12433 -0.016997 -0.070833 0.29267 -0.1151 -0.26612 -0.16511 0.19579 0.0031382 0.071859 -0.044777 -0.1025 0.3128 0.0448 0.28893 -0.13248 0.17442 0.064696 0.17354 0.091692 0.2004 0.10974 0.33205 0.085334 -0.14237 -0.18808 0.15533 0.10611 0.4627 -0.29126 -0.27753 -0.298 -0.16399 0.10904 -0.090041 -0.1876 
a 0.20184 0.20213 -0.0068384 0.022617 0.12328 0.088441 -0.41992 0.14175 0.19088 0.17783 -0.023079 0.045697 -0.44857 0.0085456 -0.18944 0.28967 0.010752 -0.060041 -0.21285 0.030452 0.075598 0.22313 0.031058 -0.0073945 -0.33162 -0.044641 0.082925 0.047076 -0.32801 0.080591 0.17454 0.11608 -0.072836 -0.0085212 -0.041314 -0.33207 0.036962 -0.10414 -0.17075 -0.066349 -0.12384 0.0067861 0.0091623 0.16404 0.26195 -0.41773 0.31712 -0.051899 0.071353 -0.046528 -0.22659 -0.098429 -0.1104 0.16071 -0.29572 -0.094333 -0.1087 0.02074 0.083292 0.0092326 0.22107 0.16124 0.12575 0.22289 0.17441 -0.022528 -0.042505 0.25714 -0.14974 -0.23864 -0.12664 0.17441 0.10994 0.042998 -0.016403 0.042373 0.23609 0.016707 0.21512 0.027021 0.22128 -0.0038756 0.26512 0.10322 0.25804 -0.024487 0.28698 0.04929 -0.13001 -0.040532 0.12707 -0.018822 0.45939 -0.24385 -0.35178 -0.27267 -0.25421 0.13457 -0.041762 -0.22118\n")
        for word_it in words_it:
            try:
                embedding = model_pretrained_it[word_it]
                f_out.write(f"{word_it} {' '.join(map(str, embedding))}\n")
            except KeyError:
                continue


def generate_word_embeddings(corpus_path, output_path):
    command = ['./fastText/fasttext', 'skipgram', '-input', corpus_path, '-output', output_path, '-minCount', '1', '-wordNgrams', '1', '-minn', '0', '-maxn', '0', '-dim', '200']
    subprocess.run(command)

def generate_crossLingual_map_embeddings(src_emb, trg_emb, src_mapped_emb, trg_mapped_emb):
    command = [
        'python3', './vecmap/map_embeddings.py', '--acl2018',
        src_emb, trg_emb, src_mapped_emb, trg_mapped_emb
    ]
    subprocess.run(command)


def load_embeddings(file_path):
    embeddings = {}
    with open(file_path, 'r', encoding='utf-8') as file:
        next(file)
        for line in file:
            parts = line.strip().split()
            word = parts[0]
            vec = np.array(parts[1:], dtype=float)
            embeddings[word] = vec
    return embeddings

def generate_dictionary(src_embeddings, trg_embeddings):
    #convertir les embedding en matrice de vecteurs de mots
    src_words, src_vecs = zip(*src_embeddings.items())
    trg_words, trg_vecs = zip(*trg_embeddings.items())
    src_matrix = np.array(src_vecs)
    trg_matrix = np.array(trg_vecs)
    
    cosine_similarities = 1 - distance.cdist(src_matrix, trg_matrix, 'cosine')
    
    best_matches = np.argmax(cosine_similarities, axis=1)
    
    dictionary = {src_word: trg_words[best_index] for src_word, best_index in zip(src_words, best_matches)}
    return dictionary

def write_dictionary_to_file(dictionary, file_path):
    with open(file_path, 'w', encoding='utf-8') as file:
        for src, trg in dictionary.items():
            file.write(src + "\t" + trg + "\n")

def parse_arguments():
    parser = argparse.ArgumentParser(description="Lexicon Induction IT-EN")
    parser.add_argument("--source_corpus", required=True, help="Chemin vers le corpus de texte source")
    parser.add_argument("--target_corpus", required=True, help="Chemin vers le corpus de texte cible")
    return parser.parse_args()


if __name__ == "__main__":

    # Analyser les arguments en ligne de commande
    args = parse_arguments()

    clean_corpus_source_path = "clean_corpus_source.txt"
    clean_corpus_target_path = "clean_corpus_target.txt"

    print("--------------------- Preprocessing du corpus de texte source -------------------------\n")
    # preprocessing source corpus
    # preprocess_text(args.source_corpus, clean_corpus_source_path)

    print("--------------------- Preprocessing du corpus de texte Cible -------------------------\n")

    # preprocessing target corpus
    preprocess_text(args.target_corpus, clean_corpus_target_path)

    print("--------------------- Génération des embeddings monolingue pour le corpus source -------------------------\n")
    source_output_path = "source_embeddings"
    generate_word_embeddings(clean_corpus_source_path, source_output_path)
    words_it = get_W2V_words_from_corpus(clean_corpus_source_path)
    words_en = get_W2V_words_from_corpus(clean_corpus_target_path)
    generate_pretrained_w2v_it_en(words_it, words_en)
    print("--------------------- Génération des embeddings monolingue pour le corpus target -------------------------\n")
    target_output_path = "target_embeddings"
    generate_word_embeddings(clean_corpus_target_path, target_output_path)

    # generation des embeddings multilingue avec vec2map
    source_cross_path = "source_crosslingual.vec"
    target_cross_path = "target_crosslingual.vec"

    ext = ".vec"
    print("--------------------- Génération des embeddings multilingue pour les corpus source & cible -------------------------\n")
    generate_crossLingual_map_embeddings(source_output_path+ext , target_output_path+ext, source_cross_path, target_cross_path)

    print("--------------------- Induction de lexique  -------------------------\n")
    source = load_embeddings(source_cross_path)
    target = load_embeddings(target_cross_path)
    Dictionnary = generate_dictionary(source, target)
    write_dictionary_to_file(Dictionnary, "Dico_IT-EN.txt")
